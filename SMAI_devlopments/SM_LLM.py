
# -*- coding: utf-8 -*-
import os
import pandas as pd
from datetime import datetime
from rapidfuzz import process, fuzz
from metaphone import doublemetaphone
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

load_dotenv()


class AcademicChatbot:
    def __init__(self, csv_path):
        self.df = pd.read_csv(csv_path)
        self.current_date = datetime(2025, 2, 23)
        self.synonym_map = {
            'ë“±ë¡ê¸ˆ': ['ë“±ë¡'],
            'ì¡¸ì—…ì‹': ['í•™ìœ„ìˆ˜ì—¬ì‹'],
            'ìˆ˜ê°•ì‹ ì²­': ['ì¥ë°”êµ¬ë‹ˆ', 'êµì°¨ìˆ˜ê°•', 'ìˆ˜ê°•ì •ì •'],
            'ê°œê°•ì´íšŒ': ['ê°œê°•', 'í•™ì‚¬ì¼ì •íšŒì˜'],
            'ì¤‘ê°„ê³ ì‚¬': ['ì¤‘ê°„ì‹œí—˜', 'ì¤‘ê°„í‰ê°€'],
            'ê¸°ë§ê³ ì‚¬': ['ê¸°ë§ì‹œí—˜', 'ê¸°ë§í‰ê°€']
        }

        self._preprocess_dates()
        self._add_status_column()

    def _preprocess_dates(self):
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ê¸°ê°„ ê³„ì‚°"""
        self.df['Start'] = pd.to_datetime(self.df['Start'])
        self.df['End'] = pd.to_datetime(self.df['End'])
        self.df['Duration'] = (self.df['End'] - self.df['Start']).dt.days + 1

    def _add_status_column(self):
        """í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ ìƒíƒœ ì»¬ëŸ¼ ì¶”ê°€"""
        self.df['Status'] = self.df.apply(
            lambda row: self._get_schedule_status(row['Start'], row['End']), axis=1
        )

    def _get_schedule_status(self, start, end):
        """ì¼ì • ìƒíƒœ íŒë³„ ë¡œì§"""
        if self.current_date < start:
            delta = (start - self.current_date).days
            return f"D-{delta} ì˜ˆì •"
        elif start <= self.current_date <= end:
            return "ì§„í–‰ ì¤‘"
        else:
            delta = (self.current_date - end).days
            return f"ì¢…ë£Œ (D+{delta})"

    def _expand_query(self, query):
        """ë™ì  ì¿¼ë¦¬ í™•ì¥ ì‹œìŠ¤í…œ"""
        # 1ë‹¨ê³„: ì§ì ‘ ë§¤ì¹­
        if query in self.synonym_map:
            return self.synonym_map[query]

        # 2ë‹¨ê³„: ìŒìš´ë¡ ì  ìœ ì‚¬ë„
        terms = list(self.synonym_map.keys()) + [item for sublist in self.synonym_map.values() for item in sublist]
        best_match = process.extractOne(query, terms, scorer=fuzz.WRatio)

        return self.synonym_map.get(best_match[0], [query]) if best_match[1] > 80 else [query]

    def _find_related_events(self, terms):
        """ê´€ë ¨ ì´ë²¤íŠ¸ ê²€ìƒ‰ ì—”ì§„"""
        results = pd.DataFrame()
        for term in terms:
            mask = self.df['Title'].str.contains(term)
            results = pd.concat([results, self.df[mask]])
        return results.drop_duplicates().sort_values(by='Start')

    def _format_response(self, events):
        """ì´ë²¤íŠ¸ í¬ë§·íŒ…"""
        response = []
        for _, row in events.iterrows():
            emoji = "ğŸŸ¢" if "ì˜ˆì •" in row['Status'] else "ğŸŸ¡" if "ì§„í–‰" in row['Status'] else "ğŸ”´"
            response.append(
                f"{emoji} {row['Title']}\n"
                f"   â–¸ ê¸°ê°„: {row['Start'].strftime('%Y-%m-%d')} ~ {row['End'].strftime('%Y-%m-%d')}\n"
                f"   â–¸ ìƒíƒœ: {row['Status']}\n"
            )
        return "\n".join(response)

    def generate_answer(self, query):
        """Ollama ê¸°ë°˜ ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±"""
        # 1. ì¿¼ë¦¬ í™•ì¥
        expanded_terms = self._expand_query(query)

        # 2. ì´ë²¤íŠ¸ ê²€ìƒ‰
        events = self._find_related_events(expanded_terms)

        # 3. ì‘ë‹µ ìƒì„±
        if not events.empty:
            formatted_events = self._format_response(events)
            prompt = f"""
            [í˜„ì¬ ë‚ ì§œ] 2025-02-23
            [ì‚¬ìš©ì ì§ˆë¬¸] {query}
            [ê²€ìƒ‰ ê²°ê³¼]
            {formatted_events}

            [ìƒì„± ê·œì¹™]
            1. ì¹œì ˆí•œ ì–´ì¡°ë¡œ ë°˜ë§ ê¸ˆì§€
            2. ëª¨ë“  ì´ë²¤íŠ¸ ë²ˆí˜¸ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ì—´
            3. ê°€ì¥ ê°€ê¹Œìš´ ì¼ì •ì€ ğŸ’¡ë¡œ ê°•ì¡°
            4. ì§€ë‚œ ì¼ì •ì€ íšŒìƒ‰ ì´ëª¨ì§€ ì‚¬ìš©
            5. ì •í™•í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” í•™ì‚¬í–‰ì •íŒ€ ì•ˆë‚´
            """

            llm = ChatOllama(
                model="exaone3.5:latest",
                temperature=0.2,
                num_ctx=4096
            )
            return llm.invoke(prompt).content

        return f"âš ï¸ ê´€ë ¨ ì¼ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ì‚¬í–‰ì •íŒ€(02-1234-5678)ìœ¼ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."

    def run(self):
        """ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤"""
        print("ìƒëª…ëŒ€í•™êµ í•™ì‚¬ì•ˆë‚´ ì±—ë´‡ ì„œë¹„ìŠ¤ ì‹œì‘\n")
        while True:
            try:
                query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
                if query.lower() == 'exit':
                    break
                print(f"\n{self.generate_answer(query)}\n")
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    chatbot = AcademicChatbot("hagsailjeong.csv")
    chatbot.run()
